{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3533061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Obtaining dependency information for torchdata from https://files.pythonhosted.org/packages/35/b2/7ed3a80ae0673b940f2af14281dc02dee0f667c6094e6dcd399fa35249a7/torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torchdata) (2.2.1)\n",
      "Requirement already satisfied: requests in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torchdata) (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests->torchdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests->torchdata) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests->torchdata) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2->torchdata) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Downloading torchdata-0.7.1-cp311-cp311-macosx_11_0_arm64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchdata\n",
      "Successfully installed torchdata-0.7.1\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/c4/c5/1a4556a372ce1bd53f183d583126a6535cae6baa1b09b7028faf018c8a67/spacy-3.7.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading spacy-3.7.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/7a/05/4a3b5c3043c6d84c00bf0f574d326660702b1c10174fe6b44cef3c3dff08/murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/d7/f6/67babf1439cdd6d46e4e805616bee84981305c80e562320c293712f54034/cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/a8/b3/1a73ba16bab53043fd19dd0a7838ae05c705dccb329404dd4ad5925767f1/preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.2.2 from https://files.pythonhosted.org/packages/05/48/2cf60744d60d07d789ce5cf6230fe2140612bc3f8ae70a89bc980ea27a28/thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/1b/d7/0800af1a75008b3a6a6a24f3efd165f2d2208076e9b8a4b11b66f16217f3/srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/fe/c3/0d04d248624a181e57c2870127dfa8d371973561caf54333c85e8f9133a2/langcodes-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/a8/73/0a9d4e7f6e78ef270e3a4532b17e060a02087590cf615ba9943fd1a283e9/blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.4-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.4/488.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.1/781.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 wasabi-1.1.2 weasel-0.3.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.1)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting pdfplumber\n",
      "  Obtaining dependency information for pdfplumber from https://files.pythonhosted.org/packages/ee/ec/315015b6c4667d889cc14348c93e31f9fd43a6f61162b85c39e4a21018d5/pdfplumber-0.11.0-py3-none-any.whl.metadata\n",
      "  Downloading pdfplumber-0.11.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Obtaining dependency information for pdfminer.six==20231228 from https://files.pythonhosted.org/packages/eb/9c/e46fe7502b32d7db6af6e36a9105abb93301fa1ec475b5ddcba8b35ae23a/pdfminer.six-20231228-py3-none-any.whl.metadata\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pdfplumber) (9.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Obtaining dependency information for pypdfium2>=4.18.0 from https://files.pythonhosted.org/packages/b8/31/1645deba5ce324afc88db614b9efcdaed0906de7704bd97929592fade355/pypdfium2-4.28.0-py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pypdfium2-4.28.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.28.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.28.0\n",
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40702 sha256=7d91847a712f06bf854420a035a2dc7248e77266b89dce38c098b071b2c9e8f6\n",
      "  Stored in directory: /Users/karthiksvasisht/Library/Caches/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n",
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchdata # 0.5.1\n",
    "!pip install -U spacy # 3.7.2\n",
    "!pip install -Uqq portalocker # 2.7.0\n",
    "!pip install -qq torchtext # 0.14.1\n",
    "!pip install -qq torchdata # 0.5.1\n",
    "!pip install -Uq nltk #3.8.1\n",
    "!python -m spacy download de\n",
    "!python -m spacy download en\n",
    "!pip install pdfplumber # 0.9.0\n",
    "!pip install fpdf # 1.7.2\n",
    "\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f88015",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m---> 20\u001b[0m     download_file(url)\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n\u001b[1;32m     11\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename=''):\n",
    "    if not filename:\n",
    "        filename = url.split('/')[-1]\n",
    "    # Stream the download to reduce memory usage\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "urls = [\n",
    "    'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py',\n",
    "    'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt',\n",
    "    'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    download_file(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e97fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05eb76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b49c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0fbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, _ = get_translation_dataloaders(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b8a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x2990dd290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_itr=iter(train_dataloader)\n",
    "data_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1f5a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthiksvasisht/Desktop/VS code/Multi30K_de_en_dataloader.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
      "/Users/karthiksvasisht/Desktop/VS code/Multi30K_de_en_dataloader.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "for n in range(1000):\n",
    "    german, english= next(data_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c48e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "german=german.T\n",
    "english=english.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744cb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0\n",
      "german input\n",
      "<bos> Zwei Welpen spielen im grünen Gras . <eos>\n",
      "english target\n",
      "<bos> Two puppies are playing in the green grass . <eos>\n",
      "_________\n",
      "\n",
      "sample 1\n",
      "german input\n",
      "<bos> Ein Mädchen liegt in einer Schneehöhle . <eos>\n",
      "english target\n",
      "<bos> A young girls is laying in a snow cave <eos>\n",
      "_________\n",
      "\n",
      "sample 2\n",
      "german input\n",
      "<bos> Männer hinter Fenstern mit Stäben davor . <eos>\n",
      "english target\n",
      "<bos> Men behind windows with bars on them . <eos>\n",
      "_________\n",
      "\n",
      "sample 3\n",
      "german input\n",
      "<bos> Ein Mädchen klettert auf einen Maschendrahtzaun . <eos>\n",
      "english target\n",
      "<bos> A girl climbing a chain link fence . <eos>\n",
      "_________\n",
      "\n",
      "sample 4\n",
      "german input\n",
      "<bos> Ein Mitglied einer Band spielt Keyboard . <eos>\n",
      "english target\n",
      "<bos> A member of a band is playing a keyboard . <eos>\n",
      "_________\n",
      "\n",
      "sample 5\n",
      "german input\n",
      "<bos> Zwei schwarz-braune Hunde blicken einander an . <eos>\n",
      "english target\n",
      "<bos> Two black and brown dogs looking at each other . <eos>\n",
      "_________\n",
      "\n",
      "sample 6\n",
      "german input\n",
      "<bos> Zwei Frauen mit Sandalen fahren Motorrad . <eos>\n",
      "english target\n",
      "<bos> Two women wearing open toed shoes are riding a motorcycle . <eos>\n",
      "_________\n",
      "\n",
      "sample 7\n",
      "german input\n",
      "<bos> Eine Personengruppe sitzt an einem Tisch . <eos>\n",
      "english target\n",
      "<bos> A group of people are enjoying themselves at a table . <eos>\n",
      "_________\n",
      "\n",
      "sample 8\n",
      "german input\n",
      "<bos> Diese Dame singt in einem Club . <eos>\n",
      "english target\n",
      "<bos> This lady is singing in a club . <eos>\n",
      "_________\n",
      "\n",
      "sample 9\n",
      "german input\n",
      "<bos> Eine Personengruppe wartet unter der Autobahn . <eos>\n",
      "english target\n",
      "<bos> A group of people waiting around under the freeway . <eos>\n",
      "_________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "    print(\"sample {}\".format(n))\n",
    "    print(\"german input\")\n",
    "    print(index_to_german(german))\n",
    "    print(\"english target\")\n",
    "    print(index_to_eng(english))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9c942e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ae2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7eab8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badabda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional information to the input tokens\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba19d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf30894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        outs =outs.to(DEVICE)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c814df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ff2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c330cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(100):\n",
    "    src ,tgt= next(data_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6755154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engish target <bos> Farmer 's selling preserves out of a van . <eos>\n",
      "german input <bos> Landwirte verkaufen Eingemachtes aus einem Van . <eos>\n"
     ]
    }
   ],
   "source": [
    "print(\"engish target\",index_to_eng(tgt))\n",
    "print(\"german input\",index_to_german(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df31905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = src.shape[0]\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a804325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )\n",
    "src_mask[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d08447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1])\n",
      "torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "src_=src[:,0].unsqueeze(1)\n",
    "print(src_.shape)\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f85b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = transformer.encode(src_, src_mask)\n",
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de7ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e2e7152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b10f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the current sequence using the transformer and retrieve the output.\n",
    "out = transformer.decode(ys, memory, tgt_mask)\n",
    "out = out.transpose(0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "090d7de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37fe7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English output: An\n"
     ]
    }
   ],
   "source": [
    "# Get the word probabilities for the last predicted word.\n",
    "prob = transformer.generator(out[:, -1])\n",
    "# Find the word index with the highest probability.\n",
    "_, next_word_index = torch.max(prob, dim=1)\n",
    "# Print the predicted English word.\n",
    "print(\"English output:\", index_to_eng(next_word_index))\n",
    "# Convert the tensor value to a Python scalar.\n",
    "next_word_index = next_word_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94d64836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engish output <bos> An\n"
     ]
    }
   ],
   "source": [
    "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
    "print(\"engish output\",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8aca1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaefc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "src\n",
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5af13741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=src.shape[0]+5\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4535f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engish  <bos> Vendors selling selling goods shop . <eos>\n"
     ]
    }
   ],
   "source": [
    "ys=greedy_decode(transformer, src, src_mask, max_len, start_symbol=BOS_IDX)\n",
    "print(\"engish \",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f4a0e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engish  <bos> Farmer 's selling preserves out of a van . <eos>\n"
     ]
    }
   ],
   "source": [
    "print(\"engish \",index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a8e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a483a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> Farmer 's selling preserves out of a van .\n",
      "<bos> Farmer 's selling preserves out of a van . <eos>\n"
     ]
    }
   ],
   "source": [
    "tgt_input = tgt[:-1, :]\n",
    "print(index_to_eng(tgt_input))\n",
    "print(index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "890e31ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of src_mask: torch.Size([9, 9])\n",
      "Shape of tgt_mask: torch.Size([10, 10])\n",
      "Shape of src_padding_mask: torch.Size([1, 9])\n",
      "Shape of tgt_padding_mask: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "print(f\"Shape of src_mask: {src_mask.shape}\")\n",
    "print(f\"Shape of tgt_mask: {tgt_mask.shape}\")\n",
    "print(f\"Shape of src_padding_mask: {src_padding_mask.shape}\")\n",
    "print(f\"Shape of tgt_padding_mask: {tgt_padding_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b3d1076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7d93989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " '<bos> Farmer',\n",
       " \"<bos> Farmer 's\",\n",
       " \"<bos> Farmer 's selling\",\n",
       " \"<bos> Farmer 's selling preserves\",\n",
       " \"<bos> Farmer 's selling preserves out\",\n",
       " \"<bos> Farmer 's selling preserves out of\",\n",
       " \"<bos> Farmer 's selling preserves out of a\",\n",
       " \"<bos> Farmer 's selling preserves out of a van\",\n",
       " \"<bos> Farmer 's selling preserves out of a van .\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tgt_mask)\n",
    "[index_to_eng( tgt_input[t==0])  for t in tgt_mask] #index_to_eng(tgt_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7985163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([10, 1, 10837])\n",
      "target shape torch.Size([10, 1])\n",
      "source shape  torch.Size([9, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "print(\"output shape\",logits.shape)\n",
    "print(\"target shape\",tgt_input.shape)\n",
    "print(\"source shape \",src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88ace31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Farmer', \"'s\", 'selling', 'preserves', 'out', 'of', 'a', 'van', '.', '<eos>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_out = tgt[1:, :]\n",
    "print(tgt_out.shape)\n",
    "[index_to_eng(t)  for t in tgt_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45f518ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4793,  106,  407, 9723,   78,   13,    4,  827,    5,    3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_out_flattened = tgt_out.reshape(-1)\n",
    "print(tgt_out_flattened.shape)\n",
    "tgt_out_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "467a3520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input: <bos> target: Farmer',\n",
       " \"input: <bos> Farmer target: 's\",\n",
       " \"input: <bos> Farmer 's target: selling\",\n",
       " \"input: <bos> Farmer 's selling target: preserves\",\n",
       " \"input: <bos> Farmer 's selling preserves target: out\",\n",
       " \"input: <bos> Farmer 's selling preserves out target: of\",\n",
       " \"input: <bos> Farmer 's selling preserves out of target: a\",\n",
       " \"input: <bos> Farmer 's selling preserves out of a target: van\",\n",
       " \"input: <bos> Farmer 's selling preserves out of a van target: .\",\n",
       " \"input: <bos> Farmer 's selling preserves out of a van . target: <eos>\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[\"input: {} target: {}\".format(index_to_eng( tgt_input[m==0]),index_to_eng( t))  for m,t in zip(tgt_mask,tgt_out)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d1dfcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3949, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06d47ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28b11fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_dataloader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    # Wrap train_dataloader with tqdm for progress logging\n",
    "    train_iterator = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for src, tgt in train_iterator:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "        tgt_mask = tgt_mask.to(DEVICE)\n",
    "        src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "        tgt_padding_mask = tgt_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        logits = logits.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d6c5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "036fd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d28c4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc42fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "673047e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoss=[]\n",
    "ValLoss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64b9c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/226 [00:00<?, ?it/s]/Users/karthiksvasisht/Desktop/VS code/Multi30K_de_en_dataloader.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
      "/Users/karthiksvasisht/Desktop/VS code/Multi30K_de_en_dataloader.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n",
      "/Users/karthiksvasisht/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.076, Val loss: 4.381, Epoch time = 839.547s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(transformer, optimizer, train_dataloader)\n\u001b[1;32m      7\u001b[0m     TrainLoss\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m      8\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[52], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_dataloader)\u001b[0m\n\u001b[1;32m     17\u001b[0m src_padding_mask \u001b[38;5;241m=\u001b[39m src_padding_mask\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m tgt_padding_mask \u001b[38;5;241m=\u001b[39m tgt_padding_mask\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 20\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n\u001b[1;32m     21\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_tok_emb(src))\n\u001b[1;32m     34\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_tok_emb(trg))\n\u001b[0;32m---> 35\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src_emb, tgt_emb, src_mask, tgt_mask, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m                         src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     37\u001b[0m outs \u001b[38;5;241m=\u001b[39mouts\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(outs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:208\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    207\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[0;32m--> 208\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    209\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    210\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    211\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:465\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    462\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 465\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    466\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    467\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    468\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    469\u001b[0m                  tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    470\u001b[0m                  memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:856\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 856\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    857\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:874\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    873\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 874\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(x, mem, mem,\n\u001b[1;32m    875\u001b[0m                             attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    876\u001b[0m                             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    877\u001b[0m                             is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    878\u001b[0m                             need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5336\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5335\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5336\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:4869\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   4868\u001b[0m q_proj \u001b[38;5;241m=\u001b[39m linear(q, w_q, b_q)\n\u001b[0;32m-> 4869\u001b[0m kv_proj \u001b[38;5;241m=\u001b[39m linear(k, w_kv, b_kv)\n\u001b[1;32m   4870\u001b[0m \u001b[38;5;66;03m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4871\u001b[0m kv_proj \u001b[38;5;241m=\u001b[39m kv_proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m2\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_dataloader)\n",
    "    TrainLoss.append(train_loss)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    ValLoss.append(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302881a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(TrainLoss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, TrainLoss, 'r', label='Training loss')\n",
    "plt.plot(epochs,ValLoss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b10998ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transformer_de_to_en_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m transformer\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_de_to_en_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transformer_de_to_en_model.pt'"
     ]
    }
   ],
   "source": [
    "!wget ''\n",
    "transformer.load_state_dict(torch.load('transformer_de_to_en_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9e471a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = torch.load('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# If you need to load the model's state dictionary into a specific model architecture\n",
    "# you would first define/instantiate your model architecture and then load the state dictionary as shown below\n",
    "# transformer.load_state_dict(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d32b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename=''):\n",
    "    if not filename:\n",
    "        filename = url.split('/')[-1]\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "\n",
    "# URL of the model file\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'\n",
    "\n",
    "# Download the file\n",
    "download_file(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "faa85087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f0de5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Sentence:  Nahaufnahme eines Hundes mit geöffnetem Maul . \n",
      "English Translation:  Closeup of dog in profile with mouth open . \n",
      "Model Translation:  Three people are sitting on a large beach . \n",
      "_________\n",
      "\n",
      "German Sentence:  Ein schwarzer Hund steht im Wasser . \n",
      "English Translation:  A black dog standing in the water . \n",
      "Model Translation:  A dog is standing in a white dog is standing in the water . \n",
      "_________\n",
      "\n",
      "German Sentence:  Fünf Männer stehen in einem Boot . \n",
      "English Translation:  Five men are standing in a boat . \n",
      "Model Translation:  Three men in a red shirt is standing in a red shirt . \n",
      "_________\n",
      "\n",
      "German Sentence:  Ein braun-weißer Hund rennt durchs Gras . \n",
      "English Translation:  A brown and white dog is running across the grass . \n",
      "Model Translation:  A dog is walking on a white dog is standing on a white dog . \n",
      "_________\n",
      "\n",
      "German Sentence:  Menschen laufen eine belebte Straße entlang . \n",
      "English Translation:  People are walking through a crowded street . \n",
      "Model Translation:  A young young boy is walking down a street . \n",
      "_________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "    print(\"German Sentence:\",index_to_german(german).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"English Translation:\",index_to_eng(english).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"Model Translation:\",translate(transformer,index_to_german(german)))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b917a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "\n",
    "    # calculate the BLEU score\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db029abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.42728700639623407 for  A dog playing in the snow in the snow . \n"
     ]
    }
   ],
   "source": [
    "generated_translation = translate(transformer,\"Ein brauner Hund spielt im Schnee .\")\n",
    "\n",
    "reference_translations = [\n",
    "    \"A brown dog is playing in the snow .\",\n",
    "    \"A brown dog plays in the snow .\",\n",
    "    \"A brown dog is frolicking in the snow .\",\n",
    "    \"In the snow, a brown dog is playing .\"\n",
    "\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
    "print(\"BLEU Score:\", bleu_score, \"for\",generated_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6dd92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import textwrap\n",
    "from fpdf import FPDF\n",
    "\n",
    "def translate_pdf(input_file, translator_model,output_file):\n",
    "    translated_text = \"\"\n",
    "\n",
    "    # Read the input PDF file\n",
    "    with pdfplumber.open(input_file) as pdf:\n",
    "\n",
    "\n",
    "        # Extract text from each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            text_content = page.extract_text()\n",
    "            num_pages = len(pdf.pages)\n",
    "            a4_width_mm = 210\n",
    "            pt_to_mm = 0.35\n",
    "            fontsize_pt = 10\n",
    "            fontsize_mm = fontsize_pt * pt_to_mm\n",
    "            margin_bottom_mm = 10\n",
    "            character_width_mm = 7 * pt_to_mm\n",
    "            width_text = a4_width_mm / character_width_mm\n",
    "\n",
    "            pdf = FPDF(orientation='P', unit='mm', format='A4')\n",
    "            pdf.set_auto_page_break(True, margin=margin_bottom_mm)\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(family='Courier', size=fontsize_pt)\n",
    "            # Split the text into sentences\n",
    "            sentences = text_content.split(\".\")\n",
    "\n",
    "            # Translate each sentence using the custom translator model\n",
    "            for sentence in sentences:\n",
    "                translated_sentence = translate(translator_model,sentence)\n",
    "                lines = textwrap.wrap(translated_sentence, width_text)\n",
    "\n",
    "                if len(lines) == 0:\n",
    "                    pdf.ln()\n",
    "\n",
    "                for wrap in lines:\n",
    "                    pdf.cell(0, fontsize_mm, wrap, ln=1)\n",
    "\n",
    "            pdf.output(output_file, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27a87971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the model file\n",
    "url1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'\n",
    "\n",
    "# Download the file\n",
    "download_file(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0512d73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated PDF file is saved as: output_en.pdf\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"input_de.pdf\"\n",
    "output_file = 'output_en.pdf'\n",
    "translate_pdf(input_file_path, transformer,output_file)\n",
    "print(\"Translated PDF file is saved as:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53535e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
